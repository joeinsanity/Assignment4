{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "_g2tDUKmhksb",
        "outputId": "c9363ebf-e41a-493f-fad6-eba2b9685ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 files belonging to 10 classes.\n",
            "Found 1306 files belonging to 10 classes.\n",
            "Found 10000 files belonging to 10 classes.\n",
            "Training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'accuracy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ed476dfbec00>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
          ]
        }
      ],
      "source": [
        "# Remove corrupt image files\n",
        "import os\n",
        "import glob\n",
        "\n",
        "files = glob.glob(\"Monkey Species Data/*/*/*\")\n",
        "for file in files:\n",
        "    f = open(file, \"rb\")\n",
        "    if not b\"JFIF\" in f.peek(10):\n",
        "        f.close()\n",
        "        os.remove(file)\n",
        "    else:\n",
        "        f.close()\n",
        "import os\n",
        "import os\n",
        "\n",
        "for file in files:\n",
        "    if not file.endswith(\".jpg\") and not file.endswith(\".png\") and not file.endswith(\".gif\") and not file.endswith(\".bmp\"):\n",
        "        print(f\"Invalid image file format: {file}\")\n",
        "\n",
        "for file in files:\n",
        "    if not file.endswith(\".jpg\") and not file.endswith(\".png\") and not file.endswith(\".gif\") and not file.endswith(\".bmp\"):\n",
        "        os.remove(file)\n",
        "\n",
        "# Loading Image Dataset\n",
        "from keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "training_set = image_dataset_from_directory(\n",
        "    \"drive/MyDrive/Monkey Species Data/Monkey Species Data/Training Data\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=(100, 100)\n",
        ")\n",
        "\n",
        "test_set = image_dataset_from_directory(\n",
        "    \"drive/MyDrive/Monkey Species Data/Monkey Species Data/Prediction Data\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=(100, 100),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Building CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Rescaling, Conv2D, Dense, Flatten, MaxPooling2D, Dropout,Input\n",
        "\n",
        "m = Sequential()\n",
        "m.add(Rescaling(1/255, input_shape=(100, 100, 3)))\n",
        "m.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
        "m.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "m.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "m.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "m.add(Flatten())\n",
        "m.add(Dense(128, activation=\"relu\"))\n",
        "m.add(Dropout(0.2))\n",
        "m.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "## number 2\n",
        "m2= Sequential()\n",
        "\n",
        "m2.add(Input((100,100,3)))\n",
        "m2.add(Rescaling(1/255))\n",
        "\n",
        "m2.add(Conv2D(32,kernel_size=(3,3),activation=\"relu\"))\n",
        "m2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "m2.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
        "m2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "m2.add(Flatten())\n",
        "m2.add(Dense(128,activation=\"relu\"))\n",
        "m2.add(Dropout(0.2))\n",
        "## new hidden layer\n",
        "m2.add(Dense(64,activation=\"relu\"))\n",
        "m2.add(Dropout(0.2))\n",
        "\n",
        "m2.add(Dense(10,activation=\"softmax\"))\n",
        "\n",
        "m2.compile(loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "m2.summary()\n",
        "\n",
        "# Compiling\n",
        "m.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "training_set = image_dataset_from_directory(\n",
        "    \"drive/MyDrive/Monkey Species Data/Monkey Species Data/Training Data\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=(100, 100)\n",
        ")\n",
        "# Compiling\n",
        "m2.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "training_set = image_dataset_from_directory(\n",
        "    \"drive/MyDrive/Monkey Species Data/Monkey Species Data/Training Data\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=(100, 100)\n",
        ")\n",
        "# Training\n",
        "epochs = 10\n",
        "print(\"Training.\")\n",
        "for i in range(epochs):\n",
        "    history = m.fit(training_set, epochs=0)\n",
        "    print(\"Epoch:\", i+1, \"Training Accuracy:\", history.history[\"accuracy\"])\n",
        "\n",
        "epochs = 10\n",
        "print(\"Training.\")\n",
        "for i in range(epochs):\n",
        "    history = m2.fit(training_set, epochs=0)\n",
        "    print(\"Epoch:\", i+1, \"Training Accuracy:\", history.history[\"accuracy\"])\n",
        "\n",
        "# Testing\n",
        "score = m.evaluate(test_set)\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "score = m2.evaluate(test_set)\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "# Saving and Loading Models\n",
        "m.save(\"my_cnn1.keras\")\n",
        "\n",
        "# Predicting\n",
        "from keras.preprocessing import image\n",
        "\n",
        "image_file = \"test.jpg\"\n",
        "img = image.load_img(image_file, target_size=(100, 100))\n",
        "img_arr = image.img_to_array(img)\n",
        "img_cl = img_arr.reshape(1, 100, 100, 3)\n",
        "\n",
        "score = m.predict(img_cl)\n",
        "print(score.round(3))\n",
        "\n",
        "score = m2.predict(img_cl)\n",
        "print(score.round(3))\n",
        "\n",
        "# Confusion Matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "corr = []\n",
        "for e in test_set:\n",
        "    corr = corr + list(np.argmax(e[1], axis=1))\n",
        "\n",
        "p = m.predict(test_set)\n",
        "pr = np.argmax(p, axis=1)\n",
        "confusion_matrix(corr, pr)\n",
        "\n",
        "# Fine-Tuning Pre-Trained Model\n",
        "from keras.applications import EfficientNetV2S\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "base_model = EfficientNetV2S(include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "output_layer = Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "m = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "m.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "## 2\n",
        "corr = []\n",
        "for e in test_set:\n",
        "    corr = corr + list(np.argmax(e[1], axis=1))\n",
        "\n",
        "p = m2.predict(test_set)\n",
        "pr = np.argmax(p, axis=1)\n",
        "confusion_matrix(corr, pr)\n",
        "\n",
        "# Fine-Tuning Pre-Trained Model\n",
        "from keras.applications import EfficientNetV2S\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "base_model = EfficientNetV2S(include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "output_layer = Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "from keras.models import Model\n",
        "\n",
        "m = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "m.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n"
      ]
    }
  ]
}